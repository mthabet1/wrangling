{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/DS3001/wrangling"
      ],
      "metadata": {
        "id": "LPX8H1AgxLj-",
        "outputId": "5b9326dd-98b6-457d-8fcc-a6095e37df66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LPX8H1AgxLj-",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wrangling'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 75 (delta 27), reused 13 (delta 10), pack-reused 39\u001b[K\n",
            "Receiving objects: 100% (75/75), 6.25 MiB | 14.45 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "J-nbQADf0aqy"
      },
      "id": "J-nbQADf0aqy",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb = pd.read_csv(\"/content/wrangling/assignment/data/airbnb_hw.csv\", low_memory=False)\n",
        "sharks = pd.read_csv(\"/content/wrangling/assignment/data/sharks.csv\", low_memory=False)"
      ],
      "metadata": {
        "id": "EHEyKIGZ0kU1"
      },
      "id": "EHEyKIGZ0kU1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part1\n",
        "airbnb['Price'].value_counts()\n",
        "price = airbnb['Price']\n",
        "price = price.str.replace(',','')\n",
        "price = pd.to_numeric(price)\n",
        "print(sum(price.isnull()))"
      ],
      "metadata": {
        "id": "USvCtHnv01eC",
        "outputId": "affb8a39-0ae2-4fac-c966-9c54adc57c89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "USvCtHnv01eC",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb['Price']=price\n",
        "airbnb['Price'].max()"
      ],
      "metadata": {
        "id": "itNbh3nM11BS",
        "outputId": "c63a4ab8-bbe2-4300-8779-ce05703ad314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "itNbh3nM11BS",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part2\n",
        "sharks['Type'].value_counts()"
      ],
      "metadata": {
        "id": "KdbYzoz62ALL",
        "outputId": "2a948b7b-527d-4abb-9d85-fccfd8d59126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KdbYzoz62ALL",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unprovoked             4716\n",
              "Provoked                593\n",
              "Invalid                 552\n",
              "Sea Disaster            239\n",
              "Watercraft              142\n",
              "Boat                    109\n",
              "Boating                  92\n",
              "Questionable             10\n",
              "Unconfirmed               1\n",
              "Unverified                1\n",
              "Under investigation       1\n",
              "Boatomg                   1\n",
              "Name: Type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_values = {'Watercraft': 'Sea Disaster', 'Boat': 'Sea Disaster', 'Boating': 'Sea Disaster', 'Boatomg': 'Sea Disaster'}\n",
        "sharks['Type'] = sharks['Type'].replace(new_values)\n",
        "new_values = {'Questionable': 'Invalid', 'Unconfirmed': 'Invalid', 'Unverified': 'Invalid','Under investigation': 'Invalid'}\n",
        "sharks['Type'] = sharks['Type'].replace(new_values)\n",
        "sharks['Type'].value_counts()\n",
        "\n",
        "#Kept the invalid variable as it indicates that the type of the accident is unclear or unknown\n",
        "#Grouped together all water related incidents\n",
        "#Did not touch the unprovoked or provoked categories"
      ],
      "metadata": {
        "id": "-SVE19Mo2TBD",
        "outputId": "a5504055-bf82-4d74-861c-60a0819843c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-SVE19Mo2TBD",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unprovoked      4716\n",
              "Provoked         593\n",
              "Sea Disaster     583\n",
              "Invalid          565\n",
              "Name: Type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part3\n",
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "df = pd.read_csv(url,low_memory=False) # Pandas downloads and loads the .csv file for you"
      ],
      "metadata": {
        "id": "Ygh19wJa4K0-"
      },
      "id": "Ygh19wJa4K0-",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['WhetherDefendantWasReleasedPretrial']=df['WhetherDefendantWasReleasedPretrial'].replace(9, np.nan)\n",
        "df['WhetherDefendantWasReleasedPretrial'].value_counts()\n",
        "print(sum(df['WhetherDefendantWasReleasedPretrial'].isnull()))\n",
        "#31 9's were turned into missing values"
      ],
      "metadata": {
        "id": "BlhR4EHo4qsi",
        "outputId": "4e3aa941-af5a-464b-e70a-cb164c3b0f77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BlhR4EHo4qsi",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part4\n",
        "#Missing values, not at random: For the pretrial data covered in the lecture, clean the\n",
        "#ImposedSentenceAllChargeInContactEvent variable as well as you can, and explain the choices you make.\n",
        "#(Hint: Look at the SentenceTypeAllChargesAtConvictionInContactEvent variable.)\n",
        "\n",
        "imposed = df['ImposedSentenceAllChargeInContactEvent']\n",
        "type = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
        "\n",
        "imposed = pd.to_numeric(imposed, errors='coerce')\n",
        "sum(imposed.isnull())\n",
        "\n",
        "pd.crosstab(imposed.isnull(), type)\n",
        "imposed = imposed.mask(type==4, 0)\n",
        "imposed = imposed.mask(type==9, np.nan)\n",
        "\n",
        "sum(imposed.isnull())\n",
        "df['ImposedSentenceAllChargeInContactEvent'] = imposed"
      ],
      "metadata": {
        "id": "5NpLzFip5Z00"
      },
      "id": "5NpLzFip5Z00",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWERS:\n",
        "\n",
        "1. The most recent census I believe allowed for self-response questions that were simply check on the box for all the applied. In the case of census, options usually include White, Black or African American, Asian, American Indian or Alaska Native, Native Hawaiian or Other pAcific Island, and the final option which is Some Other Race.\n",
        "\n",
        "2. At a purely surface level perspective, the census plays an important role in understanding makeup of demographics in the United States.\n",
        "\n",
        "  This can be crucial for policy makers, especially when attempting to understand the composition of their constituents. This allows for local level politicians to ensure representation (or at least it is supposed to).\n",
        "  \n",
        "  Obviously,  data quality is crucial here to ascertain the correctness of the data. Because this data can help inform policy decision or provide an understanding for social problems and disparity, it is essential to accurately identify the information of citizens.\n",
        "\n",
        "3. While I do not believe I know enough about the census, the (again surface level) importance it plays an understanding demographic composition at both a nationwide and local level is crucial for transparency of politicians and who they represent. With racial disparities far from gone, this information is crucial for policy education.\n",
        "\n",
        "   Because the surveys are conducted in person by federal officials, I would not be shocked to find out if lower income communities (which are disproportionately composed of minorities) are likely under represented in the survey.\n",
        "\n",
        "4. The Census again asks self reporting surveys where the respondent either identified as male or female. To my knowledge, I could not find any inidcation that censuses have options other than these 2.\n",
        "\n",
        "5. With self reported data especially, cleaning data becomes especially trick if you are trying to decipher what a respondnet intended to say or how it could be categorized\n",
        "\n",
        "  For example, I will assume a census is conducted where more than 2 genders are available as a reportable option. In an attempt to create insights and given a variety of self reportable options, many data scientists may think to group all non binary responses (did not report either man or woman) into one \"bucket\". This would lead to an incorrect portrayal of data. I think this example provides insight into how cleaning or simplofying data with this level of sensitivity is likely not as simple as intended.\n",
        "\n",
        "6. Imputing values introduces very big ethical concerns, especially for senesitve characteristics like race gender, etc. While algorithms are effectively taking over, the margin for error here is essentially 0 with nationwide ramifications based on what is imputed from available data. Additonally, imputing values of respondents who deliberately leave options blank (creating missing values) would create a huge moral dilemma if discovered."
      ],
      "metadata": {
        "id": "HSIP__wu98D1"
      },
      "id": "HSIP__wu98D1"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}